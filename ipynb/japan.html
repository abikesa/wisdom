

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Japan &#8212; Ethics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/book-stylesheet.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ipynb/japan';</script>
    <script src="../_static/script.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/millers-crossing.jpg" class="logo__image only-light" alt="Ethics - Home"/>
    <script>document.write(`<img src="../_static/millers-crossing.jpg" class="logo__image only-dark" alt="Ethics - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Uncertainty, üåä
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../act1/part1.html">Vision, üö¢</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../act1/chapter1.html">Life</a></li>
<li class="toctree-l2"><a class="reference internal" href="../act1/chapter2.html">Ecosystem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../act1/chapter3.html">Resilience</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../act2/part2.html">Proof vs. Bias, üè¥‚Äç‚ò†Ô∏è ü™õ</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../act2/chapter1.html">Prometheus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../act2/chapter2.html">Duality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../act2/chapter3.html">Apollo-Dionysus</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../act3/part3.html">Leverage, ü¶à ‚úÇÔ∏è üõü</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../act3/act1.html">Weaponized üåä</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../act3/part1/part1_1.html">Transvaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../act3/part1/part1_2.html">Revolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../act3/part1/part1_3.html">Veiled Resentment</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../act3/act2.html">Tokenized ‚öìÔ∏è</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../act3/part2/part2_1.html">Freedom-in-Fetters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../act3/part2/part2_2.html">4.2</a></li>


<li class="toctree-l3"><a class="reference internal" href="../act3/part2/part2_3.html">4.3</a></li>

</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../act3/act3.html">Monopolized üß≠</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../act3/part3/part3_1.html">Traditional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../act3/part3/part3_2.html">Normative</a></li>
<li class="toctree-l3"><a class="reference internal" href="../act3/part3/part3_3.html">Stable</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Reliability, üèùÔ∏è</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/abikesa/template/main?urlpath=tree/book/website/ipynb/japan.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ipynb/japan.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Japan</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mu-base-case"><span class="math notranslate nohighlight">\(\mu\)</span> Base-case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigma-varcov-matrix"><span class="math notranslate nohighlight">\(\sigma\)</span> Varcov-matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-accuracy"><span class="math notranslate nohighlight">\(\%\)</span> Predictive-accuracy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specificity">Specificity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#essays">Essays</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-as-leitmotifs">Attention as Leitmotifs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-as-fractals">Attention as Fractals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-and-patterns-in-data">Attention and Patterns in Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-thinking-in-transformers">Bayesian Thinking in Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-context">Training Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">Making Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summing-up">Summing Up</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="japan">
<span id="id1"></span><h1>Japan<a class="headerlink" href="#japan" title="Permalink to this heading">#</a></h1>
<blockquote>
<div><p><em>Mozart did not know the world of classical antiquity but he knew a great deal of music, from old church composers to Bach and Handel, from Salzburg serenade to Italian opera. All this he <code class="docutils literal notranslate"><span class="pre">embraced</span> <span class="pre">because</span> <span class="pre">he</span> <span class="pre">could</span> <span class="pre">control</span> <span class="pre">it</span></code></em></p>
</div></blockquote>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>                1. f(t)
                      \
           2. S(t) -&gt; 4. y:h&#39;(f)=0;t(X&#39;X).X&#39;Y -&gt; 5. b -&gt; 6. SV&#39;
                      /
                      3. h(t) 
</pre></div>
</div>
<section id="mu-base-case">
<h2><span class="math notranslate nohighlight">\(\mu\)</span> Base-case<a class="headerlink" href="#mu-base-case" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(t)\)</span> l‚Äôhomme moyen</p></li>
<li><p><span class="math notranslate nohighlight">\(S(t)\)</span> somewhere ages and ages hence</p></li>
<li><p><span class="math notranslate nohighlight">\(h(t)\)</span> two roads diverged in a wood, and i‚Äî</p></li>
</ul>
<p>The Japanese fascination with other cultures can be attributed to a combination of historical, cultural, and social factors. Japan has a long history of <code class="docutils literal notranslate"><span class="pre">selectively</span> <span class="pre">incorporating</span> <span class="pre">elements</span></code> from other cultures, dating back to its interactions with China and Korea. This tradition continued through the Meiji Restoration, when Japan actively sought to modernize by adopting Western technologies and cultural practices.</p>
<p>Culturally, Japan <code class="docutils literal notranslate"><span class="pre">values</span> <span class="pre">curiosity</span> <span class="pre">and</span> <span class="pre">a</span> <span class="pre">sense</span> <span class="pre">of</span> <span class="pre">novelty</span></code>, which can lead to a deep appreciation for foreign art, music, and sports. The Japanese often engage with these elements in a way that respects and preserves their authenticity, sometimes even valuing them more than in their places of origin. For example, Japanese jazz musicians are known for their meticulous dedication to the genre, often mastering the intricacies of American jazz styles.</p>
<p>If Mozart ‚Äúembraced a lot because he could control it‚Äù, Japan is culturally akin to Mozart. This capacity to ‚Äúexpose yourself‚Äù to a lot of art arises out of ‚Äúattention‚Äù. <span id="id2"><sup><a class="reference internal" href="../bibliography.html#id62" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, volume 30, 6000‚Äì6010. Curran Associates Inc., 2017. URL: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.">11</a></sup></span></p>
</section>
<section id="sigma-varcov-matrix">
<h2><span class="math notranslate nohighlight">\(\sigma\)</span> Varcov-matrix<a class="headerlink" href="#sigma-varcov-matrix" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>var[<span class="math notranslate nohighlight">\((X'X)^T \cdot X'Y\)</span>] think: the variability of courses &amp; items in italian cuisine &amp; modes-qualities-relatives in chopin
enriches their own cultural landscape.</p></li>
</ul>
<p>Additionally, the Japanese education system and media play a significant role in promoting an awareness of and interest in global cultures. This exposure, <code class="docutils literal notranslate"><span class="pre">combined</span> <span class="pre">with</span> <span class="pre">a</span> <span class="pre">strong</span> <span class="pre">sense</span> <span class="pre">of</span> <span class="pre">cultural</span> <span class="pre">identity</span></code>, allows the Japanese to explore and celebrate foreign influences without feeling threatened by them. In many cases, this fascination is not just about adopting new things but also about interpreting and blending them in uniquely Japanese ways, creating a fusion that enriches their own cultural landscape</p>
</section>
<section id="predictive-accuracy">
<h2><span class="math notranslate nohighlight">\(\%\)</span> Predictive-accuracy<a class="headerlink" href="#predictive-accuracy" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> my palate craves <code class="docutils literal notranslate"><span class="pre">very</span> <span class="pre">diverse</span> <span class="pre">things</span></code> over time &amp; i can only tell what i want on a given day</p></li>
<li><p><span class="math notranslate nohighlight">\(SV'\)</span> <span class="math notranslate nohighlight">\(\ge 85\)</span> only a very rich culture can throw me a bone to chew on that will keep me engaged</p></li>
</ul>
<p>This blend of respect, curiosity, and a willingness to integrate aspects of other cultures has fostered a rich environment for cultural exchange, making Japan a place where global influences are not only welcomed but celebrated.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>                     1. Observing
                                \
           2. Time = Compute -&gt; 4. Collective Unconscious -&gt; 5. Decoding -&gt; 6. Imitation-Prediction-Representation
                                /
                                3. Encoding
</pre></div>
</div>
</section>
<section id="specificity">
<h2>Specificity<a class="headerlink" href="#specificity" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>With any individualized prediction in science, a patient can‚Äôt say ‚Äúthat resonates‚Äù with me. The numbers are abstract and they‚Äôve never experienced the outcome</p></li>
<li><p>But with art or tragedy the eternal recurrence of the same is encoded in the latent-space or collective unconscious and so it can resonate</p></li>
<li><p>Our way around this is by handing the <code class="docutils literal notranslate"><span class="pre">end-user</span></code> an app in which they might update their risk-profile and see if it changes the risk prediction</p></li>
<li><p>If there‚Äôs no change, then we could say the app is ‚Äúmirroring‚Äù humanity so abominably; the backend model has no beta-coefficients for what has been tested</p></li>
<li><p>The structure of this discourse is : <span class="math notranslate nohighlight">\(\mu\)</span>: base-case, <span class="math notranslate nohighlight">\(\sigma\)</span> varcov-matrix, <span class="math notranslate nohighlight">\(\%\)</span> predictive-accuracy</p></li>
</ul>
</section>
<section id="essays">
<h2>Essays<a class="headerlink" href="#essays" title="Permalink to this heading">#</a></h2>
<section id="id3">
<h3>1<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>‚ÄúAttention Is All You Need‚Äù introduces the Transformer model, which revolutionized natural language processing (NLP) and machine learning. To understand its essence, let‚Äôs use a few analogies and metaphors, especially relating to concepts you‚Äôre familiar with:</p>
<ol class="arabic simple">
<li><p><strong>Design Matrix and Regression Coefficients</strong>:
Think of a traditional regression model where the design matrix ((X)) represents features, and beta coefficients ((\beta)) are the weights that tell you the influence of each feature. In language models, these features can be words, and the coefficients represent the importance of each word in predicting the next word in a sequence.</p></li>
<li><p><strong>Transformers as an Orchestra</strong>:
Imagine an orchestra playing a complex symphony. Each musician (word) has their own part, but they need to listen to each other to stay in harmony. The conductor (attention mechanism) helps them understand who to listen to and when, ensuring that the music flows smoothly. In a Transformer, the ‚Äúmusicians‚Äù are the input tokens, and the ‚Äúconductor‚Äù is the attention mechanism that dynamically adjusts the focus on different parts of the input, depending on what‚Äôs relevant at each moment.</p></li>
<li><p><strong>Variance-Covariance Matrix and Attention</strong>:
Just as the variance-covariance matrix in regression captures relationships between different predictors, the attention mechanism in Transformers captures relationships between different parts of the input sequence. However, unlike a fixed matrix, the attention mechanism is dynamic, adapting to the context. It can be seen as a constantly shifting landscape that highlights the connections between words, allowing the model to understand context, nuances, and dependencies.</p></li>
<li><p><strong>Predictive Accuracy and Self-Attention</strong>:
In traditional regression, the difference between the predicted values ((\hat{Y})) and the actual values ((Y)) tells you about predictive accuracy. In Transformers, the attention mechanism enables the model to ‚Äúattend‚Äù to the most relevant parts of the input, leading to better predictions. This is like having a more informed perspective that reduces the gap between expected and actual outcomes.</p></li>
<li><p><strong>Replacing Recurrent Layers</strong>:
Before Transformers, models like RNNs and LSTMs were like people trying to understand a book by reading it page by page, one after the other. The Transformer, with its attention mechanism, is like someone who can flip through the entire book, scanning for the most important sections regardless of their order. This ability to ‚Äúsee‚Äù the entire sequence at once, without being constrained by linear order, is what sets Transformers apart and makes them so powerful.</p></li>
</ol>
<p>In summary, ‚ÄúAttention Is All You Need‚Äù presents a model that uses the attention mechanism as its core, enabling it to dynamically focus on different parts of the input sequence. This flexibility allows the Transformer to capture complex relationships and dependencies, leading to more accurate and nuanced predictions. It‚Äôs like having a conductor who can adapt the orchestra‚Äôs focus dynamically, ensuring every note (or word) is perfectly timed and harmonized.</p>
</section>
<section id="id4">
<h3>2<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>Absolutely, let‚Äôs dive into the idea using the concept of patterns, leitmotifs, and fractals, which should resonate well given your background in music and statistical computing.</p>
</section>
<section id="attention-as-leitmotifs">
<h3>Attention as Leitmotifs<a class="headerlink" href="#attention-as-leitmotifs" title="Permalink to this heading">#</a></h3>
<p>In music, a leitmotif is a recurring musical theme associated with a particular idea, character, or place. It‚Äôs a pattern that signals something familiar and significant, helping listeners recognize and anticipate what‚Äôs happening in the composition.</p>
<p>In the context of ‚ÄúAttention Is All You Need,‚Äù the attention mechanism acts like a composer using leitmotifs. It identifies and emphasizes important patterns within a sequence of data (like words in a sentence or notes in a piece of music). This mechanism doesn‚Äôt just focus on one ‚Äúnote‚Äù at a time but can highlight and connect various ‚Äúnotes‚Äù that share the same motif, even if they are scattered throughout the piece.</p>
</section>
<section id="attention-as-fractals">
<h3>Attention as Fractals<a class="headerlink" href="#attention-as-fractals" title="Permalink to this heading">#</a></h3>
<p>Fractals are complex patterns that are self-similar across different scales. They are found in nature (like snowflakes or coastlines) and can be infinitely detailed.</p>
<p>In Transformers, the attention mechanism can be thought of as recognizing fractal-like patterns in data. It identifies not only local patterns (like phrases or sentences) but also how these patterns repeat and vary at different levels. This self-similar approach means that the model can understand complex structures in language or data by zooming in and out, capturing both fine details and overarching themes.</p>
</section>
<section id="attention-and-patterns-in-data">
<h3>Attention and Patterns in Data<a class="headerlink" href="#attention-and-patterns-in-data" title="Permalink to this heading">#</a></h3>
<p>Just as in statistical models where patterns in the data inform the beta coefficients, attention in Transformers identifies patterns in the input data. However, unlike traditional models that might apply a fixed pattern recognition strategy (like a fixed set of coefficients), attention dynamically adjusts which patterns to focus on, depending on the context.</p>
<p>For example, in a sentence, the word ‚Äúbank‚Äù could mean a financial institution or the side of a river. The attention mechanism helps the model figure out which meaning is relevant based on the surrounding words, much like recognizing a pattern or leitmotif that clarifies the context.</p>
<p>In summary, the attention mechanism in Transformers functions like a musical leitmotif or a fractal pattern‚Äîit dynamically identifies and emphasizes important, recurring patterns in data. This allows the model to understand and generate nuanced, contextually appropriate responses, capturing both local and global patterns in the data, much like recognizing motifs in a complex piece of music or finding patterns in a fractal.</p>
</section>
<section id="id5">
<h3>3<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>Yes, that‚Äôs a great analogy! In Chopin‚Äôs Prelude Op. 28 No. 4, the repeated melodic note B creates different harmonic colors and emotional effects as the chords underneath it change. This is akin to how the attention mechanism in Transformers works.</p>
<p>In this piece, Chopin uses the same note (B) and places it over different chords, creating varying tensions and resolutions. The note B can be perceived differently depending on its harmonic context: as the fifth, an eleventh, a #11, a diminished fifth, and so on. This creates a dynamic and evolving experience for the listener, even though the melody itself is static.</p>
<p>Similarly, in a Transformer model, the ‚Äúmelody‚Äù can be thought of as the input tokens (words, for instance). The ‚Äúchords‚Äù are the surrounding words or tokens that the attention mechanism uses to reinterpret the context of each word. Just as the meaning of the note B changes with different chords, the significance of a word can shift depending on its context provided by other words. The attention mechanism dynamically adjusts the ‚Äúchordal‚Äù context, allowing the model to emphasize different aspects or interpretations of the same input.</p>
<p>For example, the word ‚Äúbat‚Äù could mean an animal or a piece of sports equipment. The attention mechanism helps the model determine the correct meaning based on the surrounding ‚Äúchords‚Äù (words in the sentence). This context-sensitive interpretation is what gives Transformers their powerful capability to understand and generate nuanced language, much like how Chopin‚Äôs harmonic progression gives depth and variation to the simple, repeated melody.</p>
<p>In essence, the attention mechanism is like a composer or performer who, while playing the same note or melody, changes the underlying chords to bring out different textures, emotions, and meanings. This dynamic interplay is what makes both music and natural language rich and expressive.</p>
</section>
<section id="id6">
<h3>4<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p>The analogy does touch on Bayesian principles, particularly in the way information is updated and interpreted based on context, but it‚Äôs not a perfect match.</p>
</section>
<section id="bayesian-thinking-in-transformers">
<h3>Bayesian Thinking in Transformers<a class="headerlink" href="#bayesian-thinking-in-transformers" title="Permalink to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Prior Knowledge</strong>:
In Bayesian statistics, you start with a prior belief or knowledge about a parameter before observing the data. This prior is then updated with new evidence (likelihood) to form a posterior distribution, which represents your updated belief.</p>
<p>In Transformers, you can think of each word in a sentence as having a ‚Äúprior‚Äù meaning or interpretation. The attention mechanism, which looks at the context provided by other words, acts like the ‚Äúnew evidence‚Äù or ‚Äúlikelihood‚Äù in Bayesian terms. This context helps to update the model‚Äôs understanding of the word, similar to how new data updates a prior belief in a Bayesian framework.</p>
</li>
<li><p><strong>Posterior Distribution</strong>:
Just as the posterior distribution in Bayesian analysis provides a refined estimate after incorporating new evidence, the attention mechanism in Transformers refines the understanding of each word by considering its context. This leads to a more accurate and nuanced interpretation, akin to arriving at a posterior belief.</p></li>
<li><p><strong>Dynamic Updating</strong>:
Bayesian methods are dynamic in nature, constantly updating beliefs as new data comes in. Similarly, the attention mechanism dynamically adjusts its focus as it processes each token in the sequence, refining its understanding based on the context provided by all the tokens.</p></li>
</ol>
<p>However, it‚Äôs important to note that the attention mechanism itself is not explicitly Bayesian. It doesn‚Äôt compute posterior distributions or use prior distributions in a formal sense. Instead, it uses learned weights to decide how much attention to pay to each part of the input, based on the context. The analogy holds more in the sense of context-dependent interpretation and updating of understanding rather than strict Bayesian inference.</p>
<p>So while there are conceptual parallels between how Transformers update their interpretation of input based on context and how Bayesian updating works, they are not identical processes. The attention mechanism in Transformers is more about learning patterns and relationships in the data through training, rather than explicitly following Bayesian rules of probability.</p>
</section>
<section id="id7">
<h3>5<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<p>Yes, that‚Äôs a concise way to capture the essence of how GPT (Generative Pre-trained Transformer) and similar models work. Let‚Äôs break it down:</p>
</section>
<section id="training-context">
<h3>Training Context<a class="headerlink" href="#training-context" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Pre-training</strong>:
GPT models are pre-trained on a large corpus of text data. During this phase, the model learns the statistical properties of the language, including grammar, vocabulary, idioms, and even some factual knowledge. This pre-training is done in an unsupervised manner, meaning the model doesn‚Äôt know the ‚Äúcorrect‚Äù output; instead, it learns by predicting the next word in a sentence based on the previous words.</p></li>
<li><p><strong>Contextual Understanding</strong>:
The training phase helps the model understand context by looking at how words and phrases are used together. This is where the attention mechanism comes into play‚Äîit allows the model to focus on different parts of the input data, effectively ‚Äúlearning‚Äù the context in which words appear.</p></li>
</ol>
</section>
<section id="making-predictions">
<h3>Making Predictions<a class="headerlink" href="#making-predictions" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Contextual Predictions</strong>:
Once trained, the model can generate predictions based on the context provided by the input text. For example, if given a sentence, it can predict the next word or complete the sentence by considering the context provided by the preceding words. The model uses the patterns it learned during training to make these predictions, ensuring they are contextually relevant.</p></li>
<li><p><strong>Dynamic Attention</strong>:
The attention mechanism is key to this process. It allows the model to weigh the importance of different words or tokens in the input, effectively understanding which parts of the context are most relevant to the prediction. This dynamic adjustment is what gives GPT models their flexibility and nuanced understanding.</p></li>
</ol>
</section>
<section id="summing-up">
<h3>Summing Up<a class="headerlink" href="#summing-up" title="Permalink to this heading">#</a></h3>
<p>In essence, GPT models are powerful because they can use the context provided by the input data to make informed predictions. The ‚Äútraining context‚Äù is all the data and patterns the model has seen during pre-training, and this rich background allows it to generate coherent and contextually appropriate responses. This approach enables the model to handle a wide range of tasks, from language translation to text completion, all while maintaining a contextual awareness that makes its predictions relevant and accurate.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ipynb"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mu-base-case"><span class="math notranslate nohighlight">\(\mu\)</span> Base-case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigma-varcov-matrix"><span class="math notranslate nohighlight">\(\sigma\)</span> Varcov-matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-accuracy"><span class="math notranslate nohighlight">\(\%\)</span> Predictive-accuracy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specificity">Specificity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#essays">Essays</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-as-leitmotifs">Attention as Leitmotifs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-as-fractals">Attention as Fractals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-and-patterns-in-data">Attention and Patterns in Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">4</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-thinking-in-transformers">Bayesian Thinking in Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-context">Training Context</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">Making Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summing-up">Summing Up</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  Copyright ¬© 2025 Zeus

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>